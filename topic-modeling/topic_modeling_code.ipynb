{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f80dac",
   "metadata": {},
   "source": [
    "# 🔠 Topic Modeling with Gensim LDA\n",
    "\n",
    "This notebook demonstrates how to perform topic modeling using Gensim's LDA. It includes data preprocessing, training the model, extracting topics, and evaluating the quality of those topics.\n",
    "\n",
    "---\n",
    "## 🧭 How to Use This Notebook\n",
    "\n",
    "Some cells require **user input**, such as setting file paths or choosing how many topics the model should output. These are clearly commented with:\n",
    "\n",
    "```python\n",
    "# 🔧 USER: Change this value if needed\n",
    "```\n",
    "\n",
    "Check and update these parts before running the notebook. Everything else can be run without changes.\n",
    "\n",
    "---\n",
    "## 📂 Input File:\n",
    "- Either an **Excel file** (e.g. `Data.xlsx`) or a **CSV file** (e.g. `Data.csv`) with a column named `'content'`\n",
    "\n",
    "## 📁 Output Files:\n",
    "- `top_docs_df.csv`: Shows top documents per topic\n",
    "- `processed_data_with_topics.csv`: Shows all documents with their topic proportions\n",
    "- `top_keywords_per_topic.csv`: Top words that define each topic\n",
    "- `lda_visualization.html`: Interactive topic visualization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39048bfe",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb43326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mb582\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mb582\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mb582\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We import all necessary libraries for text processing, topic modeling, and visualization.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "\n",
    "# Download required NLTK resources (needed for tokenization and lemmatization)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb21cf4",
   "metadata": {},
   "source": [
    "## 2. Set Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff467128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 USER: Set how many topics the model should discover.\n",
    "# This determines how many themes will be generated from the text.\n",
    "NUM_TOPICS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73eec3",
   "metadata": {},
   "source": [
    "## 3. Load Dataset (Choose One: Excel or CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34362f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 USER: Use only one of the two options below. Comment out the one you don't use.\n",
    "# Make sure the file has a column named 'content'.\n",
    "\n",
    "# Option A: Load from Excel\n",
    "df = pd.read_excel('Data.xlsx')  # 🔧 Change file path if needed\n",
    "\n",
    "# Option B: Load from CSV\n",
    "# df = pd.read_csv('Data.csv')   # 🔧 Uncomment this line if using a CSV file\n",
    "\n",
    "# Remove rows where the 'content' field is empty or missing\n",
    "df = df[df['content'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3576587",
   "metadata": {},
   "source": [
    "## 4. Preprocess the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step tokenizes the text, converts it to lowercase,\n",
    "# removes stopwords (both standard and custom), and lemmatizes each word.\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define stopwords from NLTK and custom list\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "custom_stopwords = {\n",
    "    'one', 'know', 'could', 'like'\n",
    "}\n",
    "all_stopwords = nltk_stopwords.union(custom_stopwords)\n",
    "\n",
    "# Function to clean each document\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token not in all_stopwords and len(token) > 2]\n",
    "\n",
    "# Apply preprocessing function\n",
    "df['processed'] = df['content'].apply(preprocess)\n",
    "\n",
    "# Remove documents with less than 3 tokens after cleaning\n",
    "df = df[df['processed'].apply(lambda x: len(x) >= 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0514fc",
   "metadata": {},
   "source": [
    "## 5. Create Dictionary and Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946cef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim requires input in the form of a dictionary and a corpus.\n",
    "# The dictionary maps words to IDs, and the corpus maps documents to word frequencies.\n",
    "dictionary = corpora.Dictionary(df['processed'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['processed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb516f0",
   "metadata": {},
   "source": [
    "## 6. Train the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the topic modeling happens.\n",
    "# We specify the number of topics and let the model learn patterns in the text.\n",
    "\n",
    "lda_model = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    id2word=dictionary,\n",
    "    passes=50,\n",
    "    iterations=400,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e28db8",
   "metadata": {},
   "source": [
    "## 7. Assign Topics to Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc540482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives each document a distribution of topics with associated probabilities.\n",
    "df['Document Topics'] = [lda_model.get_document_topics(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7216ce73",
   "metadata": {},
   "source": [
    "## 8. View Topic Proportions in the Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb84e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average proportion of each topic across all documents.\n",
    "topic_counts = np.zeros(NUM_TOPICS)\n",
    "for doc_topics in df['Document Topics']:\n",
    "    for topic_num, prop in doc_topics:\n",
    "        topic_counts[topic_num] += prop\n",
    "topic_proportions = topic_counts / len(corpus)\n",
    "\n",
    "# Print topic proportions\n",
    "for topic_num, proportion in enumerate(topic_proportions):\n",
    "    print(f\"Topic {topic_num}: {proportion:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e3e54",
   "metadata": {},
   "source": [
    "## 9. Extract Top Documents for Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae2382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic, find the top 4 documents that represent it the most.\n",
    "top_docs_per_topic = {i: [] for i in range(NUM_TOPICS)}\n",
    "for idx, topics in enumerate(df['Document Topics']):\n",
    "    for topic_num, prop in topics:\n",
    "        top_docs_per_topic[topic_num].append((idx, prop))\n",
    "for topic_num in top_docs_per_topic:\n",
    "    top_docs_per_topic[topic_num] = sorted(top_docs_per_topic[topic_num], key=lambda x: x[1], reverse=True)[:4]\n",
    "\n",
    "top_docs_df = pd.DataFrame([\n",
    "    (topic_num, df.iloc[idx]['content'], prop)\n",
    "    for topic_num, docs in top_docs_per_topic.items()\n",
    "    for idx, prop in docs\n",
    "], columns=['Topic', 'Content', 'Proportion'])\n",
    "\n",
    "# Save result to CSV\n",
    "top_docs_df.to_csv('top_docs_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8376d",
   "metadata": {},
   "source": [
    "## 10. Save Full Topic Proportions for Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07263427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each document with a full list of topic probabilities (even very small ones).\n",
    "df['Topic Proportions'] = lda_model.get_document_topics(corpus, minimum_probability=0)\n",
    "df.to_csv('processed_data_with_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd2774",
   "metadata": {},
   "source": [
    "## 11. Show Top Keywords per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b6f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic, print and save the top 10 most representative words.\n",
    "top_keywords_per_topic = {i: lda_model.show_topic(i, 10) for i in range(NUM_TOPICS)}\n",
    "for topic_num, keywords in top_keywords_per_topic.items():\n",
    "    print(f\"\\nTopic {topic_num}:\")\n",
    "    for word, weight in keywords:\n",
    "        print(f\"{word}: {weight:.4f}\")\n",
    "\n",
    "top_keywords_df = pd.DataFrame([\n",
    "    (topic, word, weight)\n",
    "    for topic, keywords in top_keywords_per_topic.items()\n",
    "    for word, weight in keywords\n",
    "], columns=['Topic', 'Word', 'Weight'])\n",
    "\n",
    "top_keywords_df.to_csv('top_keywords_per_topic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe088e",
   "metadata": {},
   "source": [
    "## 12. Visualize Topics Using pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive visualization that shows topic relationships and top words.\n",
    "lda_visualization = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(lda_visualization, 'lda_visualization.html')\n",
    "pyLDAvis.display(lda_visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c278a61",
   "metadata": {},
   "source": [
    "## 13. Evaluate Model Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using two common metrics:\n",
    "# - Perplexity (lower is better)\n",
    "# - Coherence (higher is better)\n",
    "print(\"Perplexity:\", lda_model.log_perplexity(corpus))\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=df['processed'], dictionary=dictionary, coherence='c_v')\n",
    "print(\"Coherence Score:\", coherence_model.get_coherence())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
